{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-aware Attention-based RNN (TA-RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "import pandas as pd\n",
    "import random\n",
    "from keras.models import Sequential, Model\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import fbeta_score,accuracy_score,f1_score,roc_auc_score\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Dense, SimpleRNN, concatenate, Input, Flatten, Layer\n",
    "from keras.layers import Dropout \n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Masking\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A customized binary cross entropy loss function\n",
    "#ð¿ð‘œð‘ ð‘  = âˆ’1/ð‘ âˆ‘(ð›¼ âˆ™ (ð‘¦ âˆ™ ð‘™ð‘œð‘” ð‘¦â€²)) + ((1 âˆ’ ð›¼) âˆ™ (1 âˆ’ ð‘¦) âˆ™ ð‘™ð‘œð‘”(1 âˆ’ ð‘¦â€²))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to panelize positive (converter) misclassification\n",
    "def binary_cross_entropy(y, yhat):\n",
    "    epsilon = 0.7\n",
    "    loss = -(tf.math.reduce_mean((epsilon * y * tf.math.log(yhat + 1e-6)) + ((1.0- epsilon) * (1 - y) * tf.math.log(1 - yhat + 1e-6)), axis=-1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time embedding layer\n",
    "class Time_embedding_layer(Layer):\n",
    "    def __init__(self, d_model, max_seq_len):\n",
    "        super(Time_embedding_layer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def get_time_encoding_table(self, max_seq_len, d_model, t_set):\n",
    "        def reverse_min_max_normalization(normalized_value, min_val, max_val):\n",
    "            original_value = normalized_value * (max_val - min_val) + min_val\n",
    "            return original_value\n",
    "            \n",
    "        def cal_angle(el_time, hid_idx):\n",
    "            el_time = reverse_min_max_normalization(el_time, 0.5, 5)\n",
    "            return el_time / np.power(max_seq_len, 2 * (hid_idx // 2) / d_model)\n",
    "\n",
    "        def get_timei_angle_vec(time_value):\n",
    "            angels = [cal_angle(time_value, hid_j) for hid_j in range(d_model)]\n",
    "            angels = tf.convert_to_tensor(angels, dtype=tf.float32)\n",
    "            return angels\n",
    "\n",
    "        time_encoding_matrix = tf.map_fn(lambda time_i: get_timei_angle_vec(time_i), t_set, fn_output_signature=tf.float32)\n",
    "        \n",
    "        # Calculate sine of the elements in dim 2i (even indices)\n",
    "        sin_values = tf.math.sin(time_encoding_matrix[:, 0::2, :])\n",
    "        \n",
    "        # Calculate cosine of the elements in dim 2i+1 (odd indices)\n",
    "        cos_values = tf.math.cos(time_encoding_matrix[:, 1::2, :])\n",
    "        \n",
    "        # Concatenate sin_values and cos_values and store them in time_encoding_matrix\n",
    "        time_encoding_matrix = tf.concat([sin_values, cos_values], axis=1)\n",
    "        \n",
    "        return time_encoding_matrix\n",
    "\n",
    "    def time_encoding(self, time_data):\n",
    "        batch_size = tf.shape(time_data)[0]\n",
    "        time_embedding = tf.zeros((batch_size, self.max_seq_len, self.d_model), dtype=tf.float32)\n",
    "        time_embedding = self.get_time_encoding_table(self.max_seq_len, self.d_model, time_data)\n",
    "        time_embedding = tf.transpose(time_embedding, perm=[0, 2, 1])\n",
    "        return time_embedding\n",
    "    \n",
    "    def call(self, data_, time_):\n",
    "        # Make embeddings relatively larger\n",
    "        data_ = data_ * math.sqrt(self.d_model)\n",
    "        time_embedding = self.time_encoding(time_)\n",
    "        \n",
    "        # Rearange the time embedding matrix even, odd... because sin and cos were concatenated previuosly.\n",
    "        temp_data_even = time_embedding[:, :, 0::2]\n",
    "        temp_data_odd = time_embedding[:, :, 1::2]\n",
    "        arranged_time_embedding = tf.concat([temp_data_even, temp_data_odd], axis=-1)\n",
    "        \n",
    "        data_ = data_ + arranged_time_embedding\n",
    "        return data_, arranged_time_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A dual-ttention layer (visits and features attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # parameters to learn W1 and b1 for visits attention\n",
    "        self.W1 = self.add_weight(name='W1', shape=(self.units, 1), initializer='glorot_uniform', trainable=True)\n",
    "        self.b1 = self.add_weight(name='b1', shape=(1,), initializer='zeros', trainable=True)\n",
    "\n",
    "        # parameters to learn W2 and b2 for features attention\n",
    "        self.W2 = self.add_weight(name='W2', shape=(self.units, input_shape[-1][-1]), initializer='glorot_uniform', trainable=True)\n",
    "        self.b2 = self.add_weight(name='b2', shape=(input_shape[-1][-1],), initializer='zeros', trainable=True)\n",
    "\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs, mask=None):\n",
    "        h, input_data = inputs\n",
    "\n",
    "        # Calculate attention between visits\n",
    "        # Apply mask to attention weights if mask is provided\n",
    "        if mask is not None:\n",
    "            # Ensure mask is broadcastable\n",
    "            mask_broadcasted = tf.expand_dims(mask[0], axis=-1)\n",
    "            h_masked = h * tf.cast(mask_broadcasted, tf.float32)\n",
    "            e = tf.matmul(h_masked, self.W1) + self.b1\n",
    "            e = tf.squeeze(e, axis=-1)\n",
    "            alpha_unmasked = tf.nn.softmax(e, axis=-1)\n",
    "            alpha_unmasked = tf.expand_dims(alpha_unmasked, axis=-1)\n",
    "            alpha = alpha_unmasked * tf.cast(mask_broadcasted, tf.float32)\n",
    "            alpha = alpha / tf.reduce_sum(alpha, axis=1, keepdims=True)\n",
    "\n",
    "        else:\n",
    "            e = tf.matmul(h, self.W1) + self.b1\n",
    "            e = tf.squeeze(e, axis=-1)\n",
    "            alpha = tf.nn.softmax(e, axis=-1)\n",
    "            alpha = tf.expand_dims(alpha_unmasked, axis=-1)\n",
    "\n",
    "        # Calculate attention between features in each visit\n",
    "        # Apply mask to attention weights if mask is provided\n",
    "        if mask is not None:\n",
    "            # Ensure mask is broadcastable to beta_unmasked\n",
    "            mask_broadcasted = tf.expand_dims(mask[0], axis=-1)\n",
    "            h_masked = h * tf.cast(mask_broadcasted, tf.float32)\n",
    "            beta_unmasked = tf.tanh(tf.matmul(h_masked, self.W2) + self.b2)\n",
    "            beta_unmasked = tf.nn.softmax(beta_unmasked, axis=-1)\n",
    "            beta = beta_unmasked * tf.cast(mask_broadcasted, tf.float32)\n",
    "            #beta = beta / tf.reduce_sum(beta, axis=1, keepdims=True)\n",
    "        else:\n",
    "            beta = tf.tanh(tf.matmul(h, self.W2) + self.b2)\n",
    "            beta = tf.nn.softmax(beta_unmasked, axis=-1)\n",
    "\n",
    "        # Compute context vector\n",
    "        if mask is not None:\n",
    "            # Ensure mask is broadcastable to beta_unmasked\n",
    "            mask_broadcasted = tf.expand_dims(mask[0], axis=-1)\n",
    "            input_data_masked = input_data * tf.cast(mask_broadcasted, tf.float32)\n",
    "            c = tf.reduce_sum(alpha * beta * input_data_masked, axis=1)\n",
    "        else:\n",
    "            c = tf.reduce_sum(alpha * beta * input_data, axis=1)\n",
    "        \n",
    "\n",
    "        return alpha, beta, c\n",
    "\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], input_shape[0][-1])\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({'units': self.units})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TA-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TA-RNN method that takes the follwing parametres:\n",
    "# cell: it represents the RNN cell will be used {'GRU', 'biGRU', 'LSTM', 'biLSTM'}\n",
    "# drout: it represents the drop out rate will be used {0, 0.1, 0.2, 0.3, 0.4, 0.5}\n",
    "# L2: it represents the L2 regularization {0.1, 0.001, 0.00001, 0.0000001}\n",
    "# ftp: it represents future time point to predict in PPAD its 1\n",
    "# hidden_s: it represents the hidden size of RNN cell\n",
    "# embedding_s: it represents the size of embedding data (in our case it will be the number of features)\n",
    "\n",
    "def TA_RNN_with_demographic(cell, drout, L2, ftp, hidden_s, embedding_s):\n",
    "    def masking_fun(tensor_data, mask_value):\n",
    "        mask_tensor = tf.math.not_equal(tensor_data, mask_value)\n",
    "        mask_tensor = mask_tensor[:,:,0:embedding_s]\n",
    "        return mask_tensor\n",
    "    \n",
    "    # define the shape of the input for both time and longitudinal data\n",
    "    input_data_shape = (time_steps, num_features_in_each_time_step)\n",
    "    input_time_shape = (time_steps)\n",
    "\n",
    "    # define the inputs for both data and elapced time\n",
    "    input_data1 = Input(shape=input_data_shape)\n",
    "    input_data = Dense(units=embedding_s, activation='linear', activity_regularizer=l2(L2))(input_data1)\n",
    "    input_time = Input(shape=input_time_shape, name='time_input')\n",
    "    \n",
    "    hidden_s2 = math.ceil(hidden_s/2)\n",
    "\n",
    "    # add time encoding to the data\n",
    "    time_encoder = Time_embedding_layer(embedding_s, time_steps)\n",
    "    embedded_data1, time_emb = time_encoder(input_data, input_time)\n",
    "    \n",
    "    embedded_data2 = tf.where(masking_fun(input_data1, -1), embedded_data1, -1)\n",
    "    embedded_data = Masking(mask_value=-1)(embedded_data2)\n",
    "    \n",
    "    # Feed embedded data into RNN model\n",
    "    if cell == 'biGRU':\n",
    "        RNN_1 = Bidirectional(GRU(hidden_s, activity_regularizer=l2(L2), return_sequences=True, activation='relu'))(embedded_data)\n",
    "        RNN_2 = Dropout(drout)(RNN_1)\n",
    "        RNN_3 = Bidirectional(GRU(hidden_s2, activity_regularizer=l2(L2), return_sequences=True, activation='relu'))(RNN_2)\n",
    "        RNN_4 = Dropout(drout)(RNN_3)\n",
    "    elif cell == 'biLSTM':\n",
    "        RNN_1 = Bidirectional(LSTM(hidden_s, activity_regularizer=l2(L2), return_sequences=True, activation='relu'))(embedded_data)\n",
    "        RNN_2 = Dropout(drout)(RNN_1)\n",
    "        RNN_3 = Bidirectional(LSTM(hidden_s2, activity_regularizer=l2(L2), return_sequences=True, activation='relu'))(RNN_2)\n",
    "        RNN_4 = Dropout(drout)(RNN_3)\n",
    "    elif cell == 'GRU':\n",
    "        RNN_1 = GRU(hidden_s, activity_regularizer=l2(L2), return_sequences=True, activation='relu')(embedded_data)\n",
    "        RNN_2 = Dropout(drout)(RNN_1)\n",
    "        RNN_3 = GRU(hidden_s2, activity_regularizer=l2(L2), return_sequences=True, activation='relu')(RNN_2)\n",
    "        RNN_4 = Dropout(drout)(RNN_3)\n",
    "    elif cell == 'LSTM':\n",
    "        RNN_1 = LSTM(hidden_s, activity_regularizer=l2(L2), return_sequences=True, activation='relu')(embedded_data)\n",
    "        RNN_2 = Dropout(drout)(RNN_1)\n",
    "        RNN_3 = LSTM(hidden_s2, activity_regularizer=l2(L2), return_sequences=True, activation='relu')(RNN_2)\n",
    "        RNN_4 = Dropout(drout)(RNN_3)\n",
    "        \n",
    "    # Apply two levels of attention visits and features to the output of RNN\n",
    "    if cell == 'biGRU' or cell == 'biLSTM':\n",
    "        attention_layer = AttentionLayer(hidden_s)\n",
    "    else:\n",
    "        attention_layer = AttentionLayer(hidden_s2)\n",
    "    alpha, beta,  context_vector = attention_layer([RNN_4, embedded_data])\n",
    "\n",
    "    #Demographic model\n",
    "    demographic_input = Input(shape=(demographic_features))\n",
    "\n",
    "    # concatenating context_vector with demographic data\n",
    "    concat = concatenate([context_vector, demographic_input], name='MLP_Input')\n",
    "\n",
    "    # MLP Classification model    \n",
    "    MLP_1 = Dense(8, activation='relu')(concat)\n",
    "    MLP_2 = Dense(4, activation='relu')(MLP_1)\n",
    "    output = Dense(1, activation='sigmoid')(MLP_2)\n",
    "    \n",
    "    model = Model(inputs=[input_data1, input_time, demographic_input], outputs=output)\n",
    "\n",
    "    model.compile(loss= binary_cross_entropy, optimizer='adam',metrics=['accuracy'])\n",
    "    #model.compile(loss= 'binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F2 Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fbeata_function method to calculate f2 score\n",
    "def overall_fbeta_function(pred, actual):\n",
    "    # reshape the output\n",
    "    if len(actual.shape) > 2:\n",
    "        actual = np.reshape(actual, (actual.shape[0], actual.shape[1]*actual.shape[2]))\n",
    "    \n",
    "    y = []\n",
    "    \n",
    "    for i in range(pred.shape[0]):\n",
    "        for j in range(pred.shape[1]):\n",
    "            if pred[i,j] > 0.5:\n",
    "                pred[i,j] = 1\n",
    "            else:\n",
    "                pred[i,j] = 0 \n",
    "    \n",
    "    for i in range(pred.shape[0]):\n",
    "        COUNTER = 0\n",
    "        while (COUNTER < actual.shape[1]):\n",
    "            if actual[i,COUNTER] != -1:\n",
    "                COUNTER+=1\n",
    "            else:\n",
    "                break\n",
    "        y.append(actual[i,COUNTER-1])\n",
    "    y = np.array(y) \n",
    "    y = np.reshape(y, (y.shape[0], 1))\n",
    "    \n",
    "    return fbeta_score(y, pred, beta=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to build TA-RNN and train it using training data and evaluate it using test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and evaluate model\n",
    "def do_TA_RNN(longitudinal_train_data, train_time, train_label, longitudinal_test_data, test_time, test_label, demographic_train_data,\n",
    "               demographic_test_data, iteration, ftp, hp_list):\n",
    "    X_train = longitudinal_train_data\n",
    "    t_train = train_time\n",
    "    y_train = train_label[:,-1]\n",
    "\n",
    "    X_test = longitudinal_test_data\n",
    "    t_test = test_time\n",
    "    y_test = test_label[:,-1]\n",
    "    \n",
    "    # hp\n",
    "    batch_size_ = int(hp_list[0])\n",
    "    epochs_ = int(hp_list[1])\n",
    "    drout = hp_list[2]\n",
    "    L2 = hp_list[3]\n",
    "    cell = hp_list[4].strip()\n",
    "    hid_size = int(hp_list[5])\n",
    "    emb_size = int(hp_list[6])\n",
    "    emb_size = X_train.shape[-1]\n",
    "    \n",
    "    model = TA_RNN_with_demographic(cell, drout, L2, ftp, hid_size, emb_size)\n",
    "    history = model.fit([X_train, t_train, demographic_train_data], y_train, epochs=epochs_, batch_size = batch_size_, verbose=0)\n",
    "    \n",
    "    #train\n",
    "    train_loss, train_acc = model.evaluate([X_train, t_train, demographic_train_data], y_train, batch_size = batch_size_, verbose=0)\n",
    "    train_pred = model.predict([X_train, t_train, demographic_train_data], verbose=0)\n",
    "    \n",
    "    #test\n",
    "    test_loss, test_acc = model.evaluate([X_test, t_test, demographic_test_data], y_test, batch_size = batch_size_, verbose=0)\n",
    "    test_pred = model.predict([X_test, t_test, demographic_test_data], verbose=0)\n",
    "    \n",
    "    # prepare results\n",
    "\n",
    "    for i in range(test_pred.shape[0]):\n",
    "        for j in range(test_pred.shape[1]):\n",
    "            if test_pred[i,j] > 0.5:\n",
    "                test_pred[i,j] = 1\n",
    "            else:\n",
    "                test_pred[i,j] = 0\n",
    "                \n",
    "    predicted_l = np.zeros((len(test_pred)))\n",
    "    real_l = np.zeros((len(y_test)))\n",
    "    dx = test_pred.shape[1] - 1\n",
    "    for i in range(len(test_pred)):\n",
    "        predicted_l[i] = test_pred[i,dx]\n",
    "    for i in range(len(y_test)):\n",
    "        real_l[i] = y_test[i,dx]\n",
    "    CM = confusion_matrix(real_l, predicted_l, labels=[0,1])\n",
    "    \n",
    "    sensitivity = CM[1,1] / (CM[1,1] + CM[1,0])\n",
    "    specificity = CM[0,0] / (CM[0,0] + CM[0,1])\n",
    "    \n",
    "    # Table of results\n",
    "    col = 'Iteration '+str(iteration)\n",
    "    metrics_results_df = pd.DataFrame(columns = [col])\n",
    "\n",
    "    metrics_results_df.loc[len(metrics_results_df)] = [round(accuracy_score(y_test[:, -1], test_pred[:, -1]), 3)]\n",
    "    metrics_results_df.loc[len(metrics_results_df)] = [round(test_loss, 3)]\n",
    "    metrics_results_df.loc[len(metrics_results_df)] = [round(roc_auc_score(y_test[:, -1], test_pred[:, -1]), 3)]\n",
    "    metrics_results_df.loc[len(metrics_results_df)] = [round(fbeta_score(y_test[:, -1], test_pred[:, -1], beta=2), 3)] \n",
    "    metrics_results_df.loc[len(metrics_results_df)] = [round(sensitivity, 3)] \n",
    "    metrics_results_df.loc[len(metrics_results_df)] = [round(specificity, 3)]  \n",
    "    metrics_results_df.loc[len(metrics_results_df)] = [round(train_acc, 3)]\n",
    "    metrics_results_df.loc[len(metrics_results_df)] = [round(train_loss, 3)]\n",
    "    metrics_results_df.loc[len(metrics_results_df)] = [round(overall_fbeta_function(train_pred, y_train), 3)]\n",
    "    \n",
    "    return metrics_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to create and return an empty dataframe for results \n",
    "def create_table():\n",
    "    # Table of results\n",
    "    TARNN_metrics_results_df = pd.DataFrame(columns = ['Metrics'])\n",
    "    TARNN_metrics_results_df.loc[len(TARNN_metrics_results_df)] = ['Accuracy (Test)']\n",
    "    TARNN_metrics_results_df.loc[len(TARNN_metrics_results_df)] = ['Loss (Test)']\n",
    "    TARNN_metrics_results_df.loc[len(TARNN_metrics_results_df)] = ['ROC_AUC (Test)']\n",
    "    TARNN_metrics_results_df.loc[len(TARNN_metrics_results_df)] = ['F-2 (Test)'] \n",
    "    TARNN_metrics_results_df.loc[len(TARNN_metrics_results_df)] = ['Sensitivity (Test)'] \n",
    "    TARNN_metrics_results_df.loc[len(TARNN_metrics_results_df)] = ['Specificity (Test)']  \n",
    "    TARNN_metrics_results_df.loc[len(TARNN_metrics_results_df)] = ['Accuracy (Train)']\n",
    "    TARNN_metrics_results_df.loc[len(TARNN_metrics_results_df)] = ['Loss (Train)']\n",
    "    TARNN_metrics_results_df.loc[len(TARNN_metrics_results_df)] = ['F-2 (Train)']\n",
    "    \n",
    "    return TARNN_metrics_results_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best hyperparameters that have been chosen by grid search optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To read a csv file that contains best hyperparameters and copy it in a dataframe\n",
    "# Hyperparameters df contains the best values of batch_size, epoch, dropout, l2, RNN cell, hidden_size, and embedding_size\n",
    "file_name = 'hp_df.csv'\n",
    "TA_RNN_hp_df = read_csv(file_name,header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epoch</th>\n",
       "      <th>dropout</th>\n",
       "      <th>l2</th>\n",
       "      <th>cell</th>\n",
       "      <th>hidden_s</th>\n",
       "      <th>embedding_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>50</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.001</td>\n",
       "      <td>GRU</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   batch_size  epoch  dropout     l2  cell  hidden_s  embedding_s\n",
       "0           8     50      0.4  0.001   GRU        16           20"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TA_RNN_hp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "TA_RNN_hp_list = list(TA_RNN_hp_df.iloc[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 50, 0.4, 0.001, ' GRU', 16, 20]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TA_RNN_hp_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpikle data\n",
    "\n",
    "# Longitudinal training data\n",
    "file_name = 'longitudinal_data_train.pkl'\n",
    "lon_data_train = pd.read_pickle(file_name)\n",
    "\n",
    "# Labels of traing data \n",
    "file_name = 'label_train.pkl'\n",
    "label_train = pd.read_pickle(file_name)\n",
    "\n",
    "# Demographic training data\n",
    "file_name = 'demographic_data_train.pkl'\n",
    "dem_data_train = pd.read_pickle(file_name)\n",
    "\n",
    "# elapsed time training data\n",
    "file_name = 'elapsed_data_train.pkl'\n",
    "time_train = pd.read_pickle(file_name)\n",
    "\n",
    "# Longitudinal test data\n",
    "file_name = 'longitudinal_data_test.pkl'\n",
    "lon_data_test = pd.read_pickle(file_name)\n",
    "\n",
    "# Labels of test data \n",
    "file_name = 'label_test.pkl'\n",
    "label_test = pd.read_pickle(file_name)\n",
    "\n",
    "# Demographic test data\n",
    "file_name = 'demographic_data_test.pkl'\n",
    "dem_data_test = pd.read_pickle(file_name)\n",
    "\n",
    "# elapsed time test data\n",
    "file_name = 'elapsed_data_test.pkl'\n",
    "time_test = pd.read_pickle(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove age from lon_data_train and lon_data_test\n",
    "for i in range(len(lon_data_train)):\n",
    "    lon_data_train[i] = lon_data_train[i][:,:,1:]\n",
    "    lon_data_test[i] = lon_data_test[i][:,:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This represents number of visits (time points) will be used in the training.\n",
    "time_steps = lon_data_test[0].shape[1]\n",
    "\n",
    "# This represents number of future visits ahead to predict \n",
    "future_time_s = label_test[0].shape[1]\n",
    "\n",
    "# This represents how many featutes in each visit (longitudinal).\n",
    "num_features_in_each_time_step = lon_data_test[0].shape[2]\n",
    "\n",
    "# This represents how many demographic featutes (cross sectional).\n",
    "demographic_features = len(dem_data_test[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features_in_each_time_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runing TA-RNN 5 times for one scenario and save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_ 1\n",
      "TA-RNN\n",
      "iteration_ 2\n",
      "TA-RNN\n",
      "iteration_ 3\n",
      "TA-RNN\n",
      "iteration_ 4\n",
      "TA-RNN\n",
      "iteration_ 5\n",
      "TA-RNN\n"
     ]
    }
   ],
   "source": [
    "if future_time_s == 1:\n",
    "    longitudinal_train_data = lon_data_train[0]\n",
    "    train_label = label_train[0]\n",
    "    longitudinal_test_data = lon_data_test[0]\n",
    "    test_label = label_test[0]\n",
    "    demographic_train_data = np.array(dem_data_train[0])\n",
    "    demographic_test_data = np.array(dem_data_test[0])\n",
    "    train_time = time_train\n",
    "    train_time = np.reshape(train_time,(train_time.shape[0], train_time.shape[1]*train_time.shape[2]))\n",
    "    test_time = time_test\n",
    "    test_time = np.reshape(test_time,(test_time.shape[0], test_time.shape[1]*test_time.shape[2]))\n",
    "\n",
    "    # HP\n",
    "    TA_RNN_hp_list = list(TA_RNN_hp_df.iloc[0,:])\n",
    "\n",
    "    TARNN_metrics_results_df = create_table()\n",
    "    for j in range(5):\n",
    "        print(\"iteration_\", j+1)\n",
    "        #TA-RNN\n",
    "        TA_RNN_result = do_TA_RNN(longitudinal_train_data, train_time, train_label, longitudinal_test_data, test_time, test_label, demographic_train_data,\n",
    "                              demographic_test_data, j+1, future_time_s, TA_RNN_hp_list)\n",
    "        TARNN_metrics_results_df = pd.concat([TARNN_metrics_results_df, TA_RNN_result], axis=1)\n",
    "        print(\"TA-RNN\")\n",
    "    # SAVE RESULTS\n",
    "    TA_RNN_scenario = str(time_steps)+'_'+str(future_time_s)+'_TARNN.csv'\n",
    "    TARNN_metrics_results_df.to_csv(TA_RNN_scenario, index = False)\n",
    "else:\n",
    "    print('Number of future visit for prediction should be 1 for TA-RNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
